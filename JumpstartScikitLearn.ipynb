{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Example Datasets Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "(150, 4)\n",
      "(442, 10)\n",
      "(1797, 64)\n",
      "(20, 3)\n"
     ]
    }
   ],
   "source": [
    "#Loadanddisplaydetailsofthepackageddatasets\n",
    "from sklearn.datasets  import load_boston \n",
    "from sklearn.datasets  import load_iris \n",
    "from sklearn.datasets  import load_diabetes \n",
    "from sklearn . datasets  import load_digits \n",
    "from sklearn . datasets  import load_linnerud \n",
    "#Bostonhousepricesdataset(13x506,reals,regression)\n",
    "boston  = load_boston ()\n",
    "print ( boston . data . shape) \n",
    "#Irisflowerdataset(4x150,reals,multi-labelclassification)\n",
    "iris  = load_iris ()\n",
    "print ( iris . data . shape)\n",
    "#Diabetesdataset(10x442,reals,regression)\n",
    "diabetes  = load_diabetes ()\n",
    "print ( diabetes . data . shape) \n",
    "#Hand-writtendigitdataset(64x1797,multi-labelclassification)\n",
    "digits  = load_digits ()\n",
    "print ( digits . data . shape) \n",
    "#Linnerudpsychologicalandexercisedataset(3x20,3x20multivariateregression)\n",
    "linnerud  = load_linnerud ()\n",
    "print ( linnerud . data . shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "Normalization refers to rescaling real valued numeric attributes into the range 0 and 1. It is useful to scale the input attributes for a model that relies on the magnitude of values, such as distance measures used in kÂ­nearest neighbors and in the preparation of coefficients in regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "#NormalizethedataattributesfortheIrisdataset.\n",
    "from sklearn.datasets  import load_iris \n",
    "from sklearn  import preprocessing \n",
    "#loadtheirisdataset\n",
    "iris  = load_iris ()\n",
    "print ( iris.data.shape) \n",
    "#separate the data from the target attributes \n",
    "X  = iris.data\n",
    "y  = iris.target \n",
    "#normalizethedataattributes \n",
    "normalized_X  = preprocessing.normalize ( X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization\n",
    "Standardization refers to shifting the distribution of each attribute to have a mean of 0 and a standard deviation of 1. It is useful to standardize attributes for a model that relies on the distribution of attributes such as Gaussian processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "#Standardize the data attributes for the Iris data set.\n",
    "from sklearn . datasets  import load_iris \n",
    "from sklearn  import preprocessing \n",
    "#loadtheIrisdataset\n",
    "iris  = load_iris () \n",
    "print (iris.data.shape) \n",
    "\n",
    "#separate the data and target attributes \n",
    "X  = iris.data\n",
    "y  = iris.target \n",
    "\n",
    "#standardize the data attributes \n",
    "standardized_X  = preprocessing.scale ( X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing & URL Import\n",
    "Data can have missing values. These are values for attributes where a measurement could not be taken or is corrupt for some reason. It is important to identify and mark this missing data. Once marked, replacement values can be prepared. This is useful because some algorithms are unable to work with or exploit missing data.\n",
    "This recipe demonstrates marking 0 values from the Pima Indians dataset as NaN and imputing the missing values with the mean of the attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n"
     ]
    }
   ],
   "source": [
    "#Mark 0 values as missing and impute with the mean\n",
    "import numpy  as np\n",
    "import urllib\n",
    "from sklearn.preprocessing  import  Imputer #LoadthePimaIndiansDiabetesdataset\n",
    "url  =  \"http://goo.gl/j0Rvxq\"\n",
    "raw_data  = urllib.urlopen ( url)\n",
    "dataset  = np.loadtxt ( raw_data , delimiter = \",\") \n",
    "print ( dataset . shape) #separatethedataandtargetattributes\n",
    "X  = dataset [:, 0 : 7]\n",
    "y  = dataset [:, 8]\n",
    "#Mark all zero values as 0\n",
    "X [ X == 0 ]= np . nan \n",
    "#Impute all missing values with the mean of the attribute \n",
    "imp  =  Imputer ( missing_values = 'NaN' , strategy = 'mean') \n",
    "imputed_X  = imp . fit_transform ( X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "The Perceptron is a primitive type of neural network that learns weights for input attributes and transfers the weighted inputs into a network output or prediction.\n",
    "This recipes shows the fitting of a Perceptron algorithm on the diabetes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-14-e7843ce35d33>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-e7843ce35d33>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    model  =  Perceptron () model.fit ( dataset.data , dataset.target) print ( model)\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Perceptron\n",
    "import numpy  as np\n",
    "from sklearn  import datasets \n",
    "from sklearn.linear_model  import  Perceptron \n",
    "#loadthediabetesdatasets\n",
    "dataset  = datasets.load_diabetes () \n",
    "#fitaPerceptronmodeltothedata\n",
    "model  =  Perceptron() model.fit ( dataset.data , dataset.target) print ( model)\n",
    "#makepredictions\n",
    "expected  = dataset.target\n",
    "predicted  = model.predict ( dataset.data) \n",
    "#summarizethefitofthemodel\n",
    "mse  = np . mean (( predicted - expected )** 2) \n",
    "print ( mse) \n",
    "print ( model . score ( dataset . data , dataset . target ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
